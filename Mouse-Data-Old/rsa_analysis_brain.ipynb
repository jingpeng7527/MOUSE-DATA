{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsatoolbox import vis\n",
    "from rsatoolbox import rdm\n",
    "import rsatoolbox\n",
    "import rsatoolbox.data as rsd \n",
    "import rsatoolbox.rdm as rsr\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import inspect\n",
    "import scipy.io\n",
    "from collections import defaultdict\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "\n",
    "from util_function import ImageDataset, get_roi_mapping, seed_everything, image_visualization_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/yuchen/human/tutorial'\n",
    "parent_submission_dir = '/home/yuchen/psy221f_project'\n",
    "\n",
    "batch_size = 32\n",
    "input_img_dim = (64,64)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "subj = 1\n",
    "roi = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"hV4\",\n",
    "       \"EBA\", \"FBA-1\", \"FBA-2\",\n",
    "       \"OFA\", \"FFA-1\", \"FFA-2\",\n",
    "       \"OPA\", \"PPA\", \"RSC\",\"OWFA\", \"VWFA-1\", \"VWFA-2\", ]\n",
    "\n",
    "freeze_weights = True  # if True, freeze the visual encoder's weights\n",
    "subset_training_data = False  # if True, only select a small proportion of the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stimulus images: 8857\n",
      "\n",
      "Validation stimulus images: 984\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(device)\n",
    "\n",
    "\n",
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "\n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)\n",
    "\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "idxs = np.arange(len(train_img_list))\n",
    "np.random.shuffle(idxs)\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1v 1154\n",
      "V1d 1819\n",
      "V2v 1519\n",
      "V2d 1417\n",
      "hV4 1296\n",
      "EBA 6237\n",
      "FBA-1 780\n",
      "FBA-2 856\n",
      "OFA 737\n",
      "FFA-1 882\n",
      "FFA-2 629\n",
      "OPA 4669\n",
      "PPA 2202\n",
      "RSC 1061\n",
      "OWFA 907\n",
      "VWFA-1 1778\n",
      "VWFA-2 892\n",
      "fmri.shape (8857, 28835)\n"
     ]
    }
   ],
   "source": [
    "data_dir = args.data_dir\n",
    "# idxs = idxs_val \n",
    "idxs = idxs_train\n",
    "\n",
    "\n",
    "train_list = np.array(train_img_list)[idxs]\n",
    "train_list = train_list[:1000]\n",
    "        \n",
    "fmri_dir = os.path.join(data_dir, 'training_split', 'training_fmri')\n",
    "        \n",
    "lfmri = np.load(os.path.join(fmri_dir, 'lh_training_fmri.npy'))\n",
    "rfmri = np.load(os.path.join(fmri_dir, 'rh_training_fmri.npy'))\n",
    "   \n",
    "fmri = np.concatenate((lfmri[:,np.where(get_roi_mapping(data_dir, 'left', roi[0]))[0]], \n",
    "                      rfmri[:,np.where(get_roi_mapping(data_dir, 'right', roi[0]))[0]]),axis=1)\n",
    "print(roi[0], fmri.shape[1])    \n",
    "            \n",
    "for i in roi[1:]:\n",
    "  temp = np.concatenate((lfmri[:,np.where(get_roi_mapping(data_dir, 'left', i))[0]], \n",
    "                        rfmri[:,np.where(get_roi_mapping(data_dir, 'right', i))[0]]),axis=1)\n",
    "  print(i, temp.shape[1])\n",
    "\n",
    "  fmri = np.concatenate((fmri, temp),axis=1)\n",
    "            \n",
    "fmri = fmri[idxs]\n",
    "\n",
    "print('fmri.shape', fmri.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/yuchen/brain_rdm_new/’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir /home/yuchen/brain_rdm_new/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "activity_map ={\n",
    "    'V1': (0,2973),\n",
    "    'V2': (2973, 5909),\n",
    "    'V4': (5909, 7205),\n",
    "    'EBA': (7205,13442 ),\n",
    "    'FBA': (13442, 15078),\n",
    "    'OFA':(15078, 15815),\n",
    "    'FFA':(15815, 17326),\n",
    "    'OPA':(17326, 21995),\n",
    "    'PPA':(21995, 24197),\n",
    "    'RSC':(24197, 25258),\n",
    "    'OWFA':(25258, 26165),\n",
    "    'VWFA':(26165, 28835)\n",
    "}\n",
    "\n",
    "brain_layer_dir = '/home/yuchen/brain_rdm_new/'\n",
    "\n",
    "for layer_idx, name in enumerate(activity_map.keys()):\n",
    "    activity = fmri[:, activity_map[name][0]:activity_map[name][1]]\n",
    "    num_neurons = activity.shape[1]\n",
    "    # measurements = activity\n",
    "    measurements = activity[:1000,: ]\n",
    "    \n",
    "    nCond = measurements.shape[0]\n",
    "    nVox = measurements.shape[1]\n",
    "\n",
    "    des = {'session': 1, 'subj': 1}\n",
    "    obs_des = {'conds': np.array(['cond_%02d' % x for x in np.arange(nCond)])}\n",
    "    chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox)])}\n",
    "    data = rsd.Dataset(measurements=measurements,\n",
    "                    descriptors=des,\n",
    "                    obs_descriptors=obs_des,\n",
    "                    channel_descriptors=chn_des)\n",
    "\n",
    "    RDM_euc = rsr.calc_rdm(data, descriptor='conds')\n",
    "    dist_matrix_brain = RDM_euc.get_matrices()\n",
    "    \n",
    "    np.save(f'{brain_layer_dir}{name}.npy', dist_matrix_brain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psy221f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
