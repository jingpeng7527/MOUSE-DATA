{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsatoolbox import vis\n",
    "from rsatoolbox import rdm\n",
    "import rsatoolbox\n",
    "import rsatoolbox.data as rsd \n",
    "import rsatoolbox.rdm as rsr\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import inspect\n",
    "import scipy.io\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import random_projection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "\n",
    "from util_function import ImageDataset, get_roi_mapping, seed_everything, image_visualization_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/yuchen/human/tutorial'\n",
    "parent_submission_dir = '/home/yuchen/psy221f_project'\n",
    "\n",
    "batch_size = 32\n",
    "input_img_dim = (64,64)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "subj = 1\n",
    "roi = [\"V1v\", \"V1d\", \"V2v\", \"V2d\", \"hV4\",\n",
    "       \"EBA\", \"FBA-1\", \"FBA-2\",\n",
    "       \"OFA\", \"FFA-1\", \"FFA-2\",\n",
    "       \"OPA\", \"PPA\", \"RSC\",\"OWFA\", \"VWFA-1\", \"VWFA-2\", ]\n",
    "\n",
    "freeze_weights = True  # if True, freeze the visual encoder's weights\n",
    "subset_training_data = False  # if True, only select a small proportion of the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stimulus images: 8857\n",
      "\n",
      "Validation stimulus images: 984\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(device)\n",
    "\n",
    "\n",
    "class argObj:\n",
    "  def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "\n",
    "    self.subj = format(subj, '02')\n",
    "    self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "    self.parent_submission_dir = parent_submission_dir\n",
    "    self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "        'subj'+self.subj)\n",
    "\n",
    "\n",
    "args = argObj(data_dir, parent_submission_dir, subj)\n",
    "\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "\n",
    "train_img_list = os.listdir(train_img_dir)\n",
    "train_img_list.sort()\n",
    "\n",
    "num_train = int(np.round(len(train_img_list) / 100 * 90))\n",
    "idxs = np.arange(len(train_img_list))\n",
    "np.random.shuffle(idxs)\n",
    "idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "\n",
    "print('Training stimulus images: ' + format(len(idxs_train)))\n",
    "print('\\nValidation stimulus images: ' + format(len(idxs_val)))\n",
    "\n",
    "train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16_bn(weights=models.VGG16_BN_Weights.IMAGENET1K_V1).to('cuda')\n",
    "# model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1).to('cuda')\n",
    "# model = models.alexnet(weights=models.AlexNet_Weights.IMAGENET1K_V1).to('cuda')\n",
    "model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1).to('cuda')\n",
    "# # model = models.densenet161(weights=models.DenseNet161_Weights.IMAGENET1K_V1).to('cuda')\n",
    "# model = models.squeezenet1_0(weights=models.SqueezeNet1_0_Weights.IMAGENET1K_V1).to('cuda')\n",
    "# model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).to('cuda')\n",
    "# model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1).to('cuda')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for _, params in model.named_parameters():\n",
    "    params.require_grad=False\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = args.data_dir\n",
    "# idxs = idxs_val\n",
    "idxs = idxs_train[:1000]\n",
    "\n",
    "\n",
    "train_list = np.array(train_img_list)[idxs]\n",
    "# train_list = np.array(train_img_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from torchvision.models import resnet50\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# def load_backbone_feature_extractor(pretrained_path=\"\"):\n",
    "#     \"\"\"Loads a pre-trained feature extractor.\n",
    "\n",
    "#     Arguments\n",
    "#     ---------\n",
    "#         pretrained_path: str, optional\n",
    "#             Path to the feature extractor's weights.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     A Pytorch network initialized with pre-trained weights.\n",
    "\n",
    "#     \"\"\"\n",
    "#     net = resnet50(weights=None if pretrained_path == \"\" else \"DEFAULT\").cuda()\n",
    "\n",
    "#     if pretrained_path != \"\":\n",
    "#         print(\"Loading pretrained weights from: \", pretrained_path)\n",
    "\n",
    "#         # original saved file with DataParallel\n",
    "#         state_dict = torch.load(pretrained_path)\n",
    "\n",
    "#         # create new OrderedDict that does not contain `module.`\n",
    "#         new_state_dict = OrderedDict()\n",
    "#         for k, v in state_dict.items():\n",
    "#             if \"module.\" in k:\n",
    "#                 name = k[7:]  # remove `module.`\n",
    "#             elif \"_feature_blocks.\" in k:\n",
    "#                 name = k.replace(\"_feature_blocks.\", \"\")\n",
    "#             else:\n",
    "#                 name = k\n",
    "#             if name in net.state_dict().keys():\n",
    "#                 new_state_dict[name] = v\n",
    "#             else:\n",
    "#                 print(\"key \", name, \" not in dict\")\n",
    "\n",
    "#         for key in net.state_dict().keys():\n",
    "#             if key not in new_state_dict.keys():\n",
    "#                 print(\"Network key \", key, \" not in dict to load\")\n",
    "\n",
    "#         net.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "#     return net\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# model = load_backbone_feature_extractor('/home/yuchen/human/swav_feature_extractor/feature_extractor/swav_800ep_pretrain.pth.tar')\n",
    "\n",
    "# # model.fc = nn.Linear(2048, 28835)\n",
    "\n",
    "# for name, params in model.named_parameters():\n",
    "#     params.requires_grad = False\n",
    "    \n",
    "# # sample_input = img_transforms(torch.rand(8, 3, 224, 224)).cuda()\n",
    "# # _ = model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 5920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34malexnet\u001b[0m/          \u001b[01;34mmobilenet_v2_new\u001b[0m/         \u001b[01;34mresnet50_swav_new\u001b[0m/\n",
      "\u001b[01;34malexnet_new\u001b[0m/      \u001b[01;34mresnet18\u001b[0m/                 \u001b[01;34mresnet_swav\u001b[0m/\n",
      "\u001b[01;34mdensenet161\u001b[0m/      \u001b[01;34mresnet18_new\u001b[0m/             \u001b[01;34msqueezenet1_0\u001b[0m/\n",
      "\u001b[01;34minceptionv3\u001b[0m/      \u001b[01;34mresnet50\u001b[0m/                 \u001b[01;34msqueezenet1_0_new\u001b[0m/\n",
      "\u001b[01;34minceptionv3_new\u001b[0m/  \u001b[01;34mresnet50_new\u001b[0m/             \u001b[01;34mvgg16\u001b[0m/\n",
      "\u001b[01;34mmobilenet_v2\u001b[0m/     \u001b[01;34mresnet50_swav_allimages\u001b[0m/  \u001b[01;34mvgg16_new\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /home/yuchen/model_rdm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/yuchen/model_rdm/inceptionv3_new/’: File exists\n"
     ]
    }
   ],
   "source": [
    "mkdir /home/yuchen/model_rdm/inceptionv3_new/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d_1a_3x3.conv\n",
      "Conv2d_1a_3x3.bn\n",
      "Conv2d_1a_3x3.relu\n",
      "Conv2d_2a_3x3.conv\n",
      "Conv2d_2a_3x3.bn\n",
      "Conv2d_2a_3x3.relu\n",
      "Conv2d_2b_3x3.conv\n",
      "Conv2d_2b_3x3.bn\n",
      "Conv2d_2b_3x3.relu\n",
      "maxpool1\n",
      "Conv2d_3b_1x1.conv\n",
      "Conv2d_3b_1x1.bn\n",
      "Conv2d_3b_1x1.relu\n",
      "Conv2d_4a_3x3.conv\n",
      "Conv2d_4a_3x3.bn\n",
      "Conv2d_4a_3x3.relu\n",
      "maxpool2\n",
      "Mixed_5b.branch1x1.conv\n",
      "Mixed_5b.branch1x1.bn\n",
      "Mixed_5b.branch1x1.relu\n",
      "Mixed_5b.branch5x5_1.conv\n",
      "Mixed_5b.branch5x5_1.bn\n",
      "Mixed_5b.branch5x5_1.relu\n",
      "Mixed_5b.branch5x5_2.conv\n",
      "Mixed_5b.branch5x5_2.bn\n",
      "Mixed_5b.branch5x5_2.relu\n",
      "Mixed_5b.branch3x3dbl_1.conv\n",
      "Mixed_5b.branch3x3dbl_1.bn\n",
      "Mixed_5b.branch3x3dbl_1.relu\n",
      "Mixed_5b.branch3x3dbl_2.conv\n",
      "Mixed_5b.branch3x3dbl_2.bn\n",
      "Mixed_5b.branch3x3dbl_2.relu\n",
      "Mixed_5b.branch3x3dbl_3.conv\n",
      "Mixed_5b.branch3x3dbl_3.bn\n",
      "Mixed_5b.branch3x3dbl_3.relu\n",
      "Mixed_5b.avg_pool2d\n",
      "Mixed_5b.branch_pool.conv\n",
      "Mixed_5b.branch_pool.bn\n",
      "Mixed_5b.branch_pool.relu\n",
      "Mixed_5c.branch1x1.conv\n",
      "Mixed_5c.branch1x1.bn\n",
      "Mixed_5c.branch1x1.relu\n",
      "Mixed_5c.branch5x5_1.conv\n",
      "Mixed_5c.branch5x5_1.bn\n",
      "Mixed_5c.branch5x5_1.relu\n",
      "Mixed_5c.branch5x5_2.conv\n",
      "Mixed_5c.branch5x5_2.bn\n",
      "Mixed_5c.branch5x5_2.relu\n",
      "Mixed_5c.branch3x3dbl_1.conv\n",
      "Mixed_5c.branch3x3dbl_1.bn\n",
      "Mixed_5c.branch3x3dbl_1.relu\n",
      "Mixed_5c.branch3x3dbl_2.conv\n",
      "Mixed_5c.branch3x3dbl_2.bn\n",
      "Mixed_5c.branch3x3dbl_2.relu\n",
      "Mixed_5c.branch3x3dbl_3.conv\n",
      "Mixed_5c.branch3x3dbl_3.bn\n",
      "Mixed_5c.branch3x3dbl_3.relu\n",
      "Mixed_5c.avg_pool2d\n",
      "Mixed_5c.branch_pool.conv\n",
      "Mixed_5c.branch_pool.bn\n",
      "Mixed_5c.branch_pool.relu\n",
      "Mixed_5d.branch1x1.conv\n",
      "Mixed_5d.branch1x1.bn\n",
      "Mixed_5d.branch1x1.relu\n",
      "Mixed_5d.branch5x5_1.conv\n",
      "Mixed_5d.branch5x5_1.bn\n",
      "Mixed_5d.branch5x5_1.relu\n",
      "Mixed_5d.branch5x5_2.conv\n",
      "Mixed_5d.branch5x5_2.bn\n",
      "Mixed_5d.branch5x5_2.relu\n",
      "Mixed_5d.branch3x3dbl_1.conv\n",
      "Mixed_5d.branch3x3dbl_1.bn\n",
      "Mixed_5d.branch3x3dbl_1.relu\n",
      "Mixed_5d.branch3x3dbl_2.conv\n",
      "Mixed_5d.branch3x3dbl_2.bn\n",
      "Mixed_5d.branch3x3dbl_2.relu\n",
      "Mixed_5d.branch3x3dbl_3.conv\n",
      "Mixed_5d.branch3x3dbl_3.bn\n",
      "Mixed_5d.branch3x3dbl_3.relu\n",
      "Mixed_5d.avg_pool2d\n",
      "Mixed_5d.branch_pool.conv\n",
      "Mixed_5d.branch_pool.bn\n",
      "Mixed_5d.branch_pool.relu\n",
      "Mixed_6a.branch3x3.conv\n",
      "Mixed_6a.branch3x3.bn\n",
      "Mixed_6a.branch3x3.relu\n",
      "Mixed_6a.branch3x3dbl_1.conv\n",
      "Mixed_6a.branch3x3dbl_1.bn\n",
      "Mixed_6a.branch3x3dbl_1.relu\n",
      "Mixed_6a.branch3x3dbl_2.conv\n",
      "Mixed_6a.branch3x3dbl_2.bn\n",
      "Mixed_6a.branch3x3dbl_2.relu\n",
      "Mixed_6a.branch3x3dbl_3.conv\n",
      "Mixed_6a.branch3x3dbl_3.bn\n",
      "Mixed_6a.branch3x3dbl_3.relu\n",
      "Mixed_6a.max_pool2d\n",
      "Mixed_6b.branch1x1.conv\n",
      "Mixed_6b.branch1x1.bn\n",
      "Mixed_6b.branch1x1.relu\n",
      "Mixed_6b.branch7x7_1.conv\n",
      "Mixed_6b.branch7x7_1.bn\n",
      "Mixed_6b.branch7x7_1.relu\n",
      "Mixed_6b.branch7x7_2.conv\n",
      "Mixed_6b.branch7x7_2.bn\n",
      "Mixed_6b.branch7x7_2.relu\n",
      "Mixed_6b.branch7x7_3.conv\n",
      "Mixed_6b.branch7x7_3.bn\n",
      "Mixed_6b.branch7x7_3.relu\n",
      "Mixed_6b.branch7x7dbl_1.conv\n",
      "Mixed_6b.branch7x7dbl_1.bn\n",
      "Mixed_6b.branch7x7dbl_1.relu\n",
      "Mixed_6b.branch7x7dbl_2.conv\n",
      "Mixed_6b.branch7x7dbl_2.bn\n",
      "Mixed_6b.branch7x7dbl_2.relu\n",
      "Mixed_6b.branch7x7dbl_3.conv\n",
      "Mixed_6b.branch7x7dbl_3.bn\n",
      "Mixed_6b.branch7x7dbl_3.relu\n",
      "Mixed_6b.branch7x7dbl_4.conv\n",
      "Mixed_6b.branch7x7dbl_4.bn\n",
      "Mixed_6b.branch7x7dbl_4.relu\n",
      "Mixed_6b.branch7x7dbl_5.conv\n",
      "Mixed_6b.branch7x7dbl_5.bn\n",
      "Mixed_6b.branch7x7dbl_5.relu\n",
      "Mixed_6b.avg_pool2d\n",
      "Mixed_6b.branch_pool.conv\n",
      "Mixed_6b.branch_pool.bn\n",
      "Mixed_6b.branch_pool.relu\n",
      "Mixed_6c.branch1x1.conv\n",
      "Mixed_6c.branch1x1.bn\n",
      "Mixed_6c.branch1x1.relu\n",
      "Mixed_6c.branch7x7_1.conv\n",
      "Mixed_6c.branch7x7_1.bn\n",
      "Mixed_6c.branch7x7_1.relu\n",
      "Mixed_6c.branch7x7_2.conv\n",
      "Mixed_6c.branch7x7_2.bn\n",
      "Mixed_6c.branch7x7_2.relu\n",
      "Mixed_6c.branch7x7_3.conv\n",
      "Mixed_6c.branch7x7_3.bn\n",
      "Mixed_6c.branch7x7_3.relu\n",
      "Mixed_6c.branch7x7dbl_1.conv\n",
      "Mixed_6c.branch7x7dbl_1.bn\n",
      "Mixed_6c.branch7x7dbl_1.relu\n",
      "Mixed_6c.branch7x7dbl_2.conv\n",
      "Mixed_6c.branch7x7dbl_2.bn\n",
      "Mixed_6c.branch7x7dbl_2.relu\n",
      "Mixed_6c.branch7x7dbl_3.conv\n",
      "Mixed_6c.branch7x7dbl_3.bn\n",
      "Mixed_6c.branch7x7dbl_3.relu\n",
      "Mixed_6c.branch7x7dbl_4.conv\n",
      "Mixed_6c.branch7x7dbl_4.bn\n",
      "Mixed_6c.branch7x7dbl_4.relu\n",
      "Mixed_6c.branch7x7dbl_5.conv\n",
      "Mixed_6c.branch7x7dbl_5.bn\n",
      "Mixed_6c.branch7x7dbl_5.relu\n",
      "Mixed_6c.avg_pool2d\n",
      "Mixed_6c.branch_pool.conv\n",
      "Mixed_6c.branch_pool.bn\n",
      "Mixed_6c.branch_pool.relu\n",
      "Mixed_6d.branch1x1.conv\n",
      "Mixed_6d.branch1x1.bn\n",
      "Mixed_6d.branch1x1.relu\n",
      "Mixed_6d.branch7x7_1.conv\n",
      "Mixed_6d.branch7x7_1.bn\n",
      "Mixed_6d.branch7x7_1.relu\n",
      "Mixed_6d.branch7x7_2.conv\n",
      "Mixed_6d.branch7x7_2.bn\n",
      "Mixed_6d.branch7x7_2.relu\n",
      "Mixed_6d.branch7x7_3.conv\n",
      "Mixed_6d.branch7x7_3.bn\n",
      "Mixed_6d.branch7x7_3.relu\n",
      "Mixed_6d.branch7x7dbl_1.conv\n",
      "Mixed_6d.branch7x7dbl_1.bn\n",
      "Mixed_6d.branch7x7dbl_1.relu\n",
      "Mixed_6d.branch7x7dbl_2.conv\n",
      "Mixed_6d.branch7x7dbl_2.bn\n",
      "Mixed_6d.branch7x7dbl_2.relu\n",
      "Mixed_6d.branch7x7dbl_3.conv\n",
      "Mixed_6d.branch7x7dbl_3.bn\n",
      "Mixed_6d.branch7x7dbl_3.relu\n",
      "Mixed_6d.branch7x7dbl_4.conv\n",
      "Mixed_6d.branch7x7dbl_4.bn\n",
      "Mixed_6d.branch7x7dbl_4.relu\n",
      "Mixed_6d.branch7x7dbl_5.conv\n",
      "Mixed_6d.branch7x7dbl_5.bn\n",
      "Mixed_6d.branch7x7dbl_5.relu\n",
      "Mixed_6d.avg_pool2d\n",
      "Mixed_6d.branch_pool.conv\n",
      "Mixed_6d.branch_pool.bn\n",
      "Mixed_6d.branch_pool.relu\n",
      "Mixed_6e.branch1x1.conv\n",
      "Mixed_6e.branch1x1.bn\n"
     ]
    }
   ],
   "source": [
    "performance_map = {}\n",
    "model_layer_dir = '/home/yuchen/model_rdm/inceptionv3_new/'\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    for layer_idx, current_feature_layer in enumerate(get_graph_node_names(model)[0][1:]):\n",
    "        \n",
    "        model_layer = current_feature_layer\n",
    "        \n",
    "        if all(x not in current_feature_layer for x in ['unsqueeze', 'mul', 'add','getitem','cat']):\n",
    "            if -1 != 0: \n",
    "                model_layer = current_feature_layer\n",
    "                print(model_layer)\n",
    "                    \n",
    "                feature_extractor = create_feature_extractor(model, return_nodes=[model_layer]).to('cuda')\n",
    "\n",
    "                # feature_map_lst = np.array([])\n",
    "                feature_map_lst = []\n",
    "                # feature_map_lst_100 = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for idx, train_img_dir_individual in enumerate(train_list):\n",
    "                        image = Image.open(f'{args.data_dir}/training_split/training_images/{train_img_dir_individual}').convert('RGB')\n",
    "\n",
    "                        input_tensor = transform(image).to('cuda')\n",
    "                        \n",
    "                        input_tensor = torch.unsqueeze(input_tensor,0)\n",
    "\n",
    "                        feature_map = feature_extractor(input_tensor)[model_layer]\n",
    "                        feature_map = feature_map.cpu().detach().numpy()\n",
    "                        feature_map = feature_map.reshape(-1)\n",
    "                        feature_map_lst.append(feature_map)\n",
    "                        \n",
    "                feature_map_lst = np.array(feature_map_lst)\n",
    "                    \n",
    "                #     if len(feature_map) > 5920:\n",
    "                #         feature_map_lst_100.append(feature_map)\n",
    "                #         if len(feature_map_lst_100) == 100:\n",
    "                #             feature_map_lst_100 = np.array(feature_map_lst_100)\n",
    "                #             transformer = random_projection.SparseRandomProjection(n_components=n_components)\n",
    "                #             x_new = transformer.fit_transform(feature_map_lst_100)\n",
    "                #             feature_map_lst = np.vstack([feature_map_lst, x_new]) if feature_map_lst.size else x_new\n",
    "                #             feature_map_lst_100 = []\n",
    "                #     else: feature_map_lst.append(feature_map)\n",
    "                    \n",
    "                # if len(feature_map) > 5920:\n",
    "                #     feature_map_lst_100 = np.array(feature_map_lst_100)\n",
    "                #     transformer = random_projection.SparseRandomProjection(n_components=n_components)\n",
    "                #     x_new = transformer.fit_transform(feature_map_lst_100)\n",
    "                #     feature_map_lst = np.vstack([feature_map_lst, x_new]) if feature_map_lst.size else x_new\n",
    "                # else: feature_map_lst = np.array(feature_map_lst)\n",
    "                    \n",
    "                measurements = feature_map_lst\n",
    "                nCond = measurements.shape[0]\n",
    "                nVox = measurements.shape[1]\n",
    "\n",
    "                des = {'session': 1, 'subj': 1}\n",
    "                obs_des = {'conds': np.array(['cond_%02d' % x for x in np.arange(nCond)])}\n",
    "                chn_des = {'voxels': np.array(['voxel_' + str(x) for x in np.arange(nVox)])}\n",
    "                data = rsd.Dataset(measurements=measurements,\n",
    "                                    descriptors=des,\n",
    "                                    obs_descriptors=obs_des,\n",
    "                                    channel_descriptors=chn_des)\n",
    "\n",
    "                RDM_euc = rsr.calc_rdm(data, descriptor='conds')\n",
    "                dist_matrix_model = RDM_euc.get_matrices()\n",
    "                    \n",
    "                np.save(f'{model_layer_dir}{str(layer_idx)}_{model_layer}.npy', dist_matrix_model)\n",
    "        \n",
    "    # correlations = np.mean(rsatoolbox.rdm.compare_correlation(dist_matrix_model, dist_matrix_brain))\n",
    "    \n",
    "    # performance_map[current_feature_layer] = correlations\n",
    "    \n",
    "    # print(correlations,end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
